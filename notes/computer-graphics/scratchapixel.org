#+TITLE: Notes from the [[http://scratchapixel.com][scratchapixel.com]]
If you see any text enclosed with \[these\] brackets, that's just some latex "code" that you can view in org-mode (in emacs). You won't be able to view it on GitHub but if you have cloned the repo, you can view it (and all the other code segments in the page) by typing the following:
#+BEGIN_SRC 
C-u C-c C-x C-l
#+END_SRC

* Introduction to Ray Tracing: a Simple Method for Creating 3D Images
** How does it work?
*** What are the steps of creating a 2d image of a 3d scene?
- Perspective Projection
  - Projecting the 3d image onto an image plane
- Light and Color
  - What is light made of? Photons
  - What are the components of photons? They are electromagnetic, so they have an electric and a magnetic component
  - If light hits an object, what three things can happen to it? It can either be reflected, absorbed or transmitted
  - What do all materials have in common? The number of incoming photons is equal to the sum of photons reflected, absorbed or transmitted
  - What are the two types of metal materials? Conductors and dielectrics
  - What are some examples of dielectrics? Glass, water, wood, plastic, etc.
  - Why does an object appear a certain color? Let's take the example of a red ball. If a white light is illuminating it (a white light consists of red, green and blue photons) the red ball will absorb the red and green components and reflect the red light. The object appears red because our eye catches the red photons that the ball reflects
** Raytracing Algorithm in a Nutshell
*** What, in simple terms, is forward ray-tracing?
- Following the path of a photon (or many photons) from the light source to the observer (the eye or an image plane)
- Why is forward ray-tracing problematic, especially in the realm of the constraints on modern computers? In terms of probability, there are very few photons that will be emitted from a light source and actually fall onto the image plane (or eye). Thus, potentially there are many wasted operations and calculations that are completed
*** What, in simple terms, is backwards ray-tracing?
- It's quite simply the opposite of forward ray-tracing. Instead of tracing every single photon/ray from the light source, we trace rays from the eye back to the light source and keep the ones that actually reach it
- When using backwards ray-tracing, what rays do we "keep" and what are they called? When we shoot a ray from the eye, if it reaches the object, we shoot another ray from that object (which follows the geometry/material construction of that object) and see if it hits the light source. This ray is called a light or shadow ray
- If the ray that gets reflected from the object then intersects another object before hitting the light, what kind of ray is it? In this case, it's technically a shadow ray since it's in the objects shadow
- What are the rays that get emitted from the eye called? Primary ray, visibility ray or camera ray
- What is path tracing? The process of either casting rays from the light source or from the eye/camera
*** Sometimes, an author may refer to forward ray-tracing when they are referring to backwards ray-tracing. You can determine this usually by the type of ray they are referring to. If they are talking about the primary ray or camera ray, they are actually talking about backwards ray-tracing
** Implementing the Raytracing Algorithm
*** Describe the raytracing algorithm in basic terms
- Step 1. For each pixel, shot a ray from the eye that intersects the pixel's center
- Step 2. Find what object the ray intersects in the scene. If it intersect multiple objects, use the closest one
- Step 3. Shoot a shadow ray from the object the ray intersected to see if it reaches the light source. If it does, the point the eye ray intersected is illuminated by the light. If not, the object that the eye ray intersected is in the shadow of the object that the shadow ray intersected. Repeat for all eye rays
*** What are some other ray geometry intersection algorithms?
- Scanline renderer and the z-buffer algorithm
*** Who, in 1969 in a paper entitled "Some Techniques for Shading Machine Renderings of Solids", first introduced the raytracing technique?
- Arthur Appel
*** Example source code for a simple raytracing algorithm[fn:1]
#+BEGIN_SRC
for (int j = 0; j < imageHeight; ++j) { 
    for (int i = 0; i < imageWidth; ++i) { 
        // compute primary ray direction
        Ray primRay; 
        computePrimRay(i, j, &primRay); 
        // shoot prim ray in the scene and search for intersection
        Point pHit; 
        Normal nHit; 
        float minDist = INFINITY; 
        Object object = NULL; 
        for (int k = 0; k < objects.size(); ++k) { 
            if (Intersect(objects[k], primRay, &pHit, &nHit)) { 
                float distance = Distance(eyePosition, pHit); 
                if (distance < minDistance) { 
                    object = objects[k]; 
                    minDistance = distance; // update min distance 
                    } 
            } 
        } 
        if (object != NULL) { 
            // compute illumination
            Ray shadowRay; 
            shadowRay.direction = lightPosition - pHit; 
            bool isShadow = false; 
            for (int k = 0; k < objects.size(); ++k) { 
                if (Intersect(objects[k], shadowRay)) { 
                    isInShadow = true; 
                    break; 
                } 
            } 
        } 
        if (!isInShadow) 
            pixels[i][j] = object->color * light.brightness; 
        else 
            pixels[i][j] = 0; 
    } 
} 
#+END_SRC
** Adding Reflection and Refraction
*** Who was the first to describe how to extend Appel's ray-tracing algorithm for more advanced rendering?
- Turner Whitted
*** What are reflection and refraction [ray] directions based on?
- The normal at the point of intersection and the direction of the incoming ray (primary ray)
*** To compute the refraction [ray] direction, what additional information do we need?
- The index of refraction (IOR) of the object being intersected
*** How, in general terms, do we determine how much of an incoming ray gets reflected and how much of it is refracted?
- Intuitively enough, it's not just a matter of splitting the difference between reflection and refraction. It is based on the look angle (the primary ray) and both the IOR and the normal of the object. Given that info, there's an equation to calculate everything and that equation is known as the Fresnel equation. More on this later
*** To recap, what three steps are involved in the Whitted equation (to determine the color of a point on an object with both refraction and reflection, like a glass ball for instance)
- Computer the reflection color, computer the refraction color and then apply the Fresnel equation
*** What is a transmission ray?
- When a ray hits an object with refractive properties, that ray will go through the object. That ray is then considered a transmission ray

* Where Do I Start? A Very Gentle Introduction to Computer Graphics Programming
** A Gentle Introduction To Computer Graphics Programming
*** What is foreshortening?
- Objects appearing smaller the farther away they are
*** What is stereoscopic vision?
- This is when our brain is able to use our two eyes with two slightly different angles of the views in front of it to create an image that we are able to approximate the distance to the objects in front of us
*** In CG, what is a scene?
- A scene is a collection of objects. Each object will have a set of coordinates that describe the object and its dimensions
*** What is topology?
- Topology refers to how points which we generally call vertices are connected to each other to form faces (or flat surfaces)
*** What is a viewing frustum?
- It's the pyramid that is created when drawing lines from the eye to the corners of the canvas as far as you need
*** In CG, what is the world coordinate system?
- It's really just the coordinate system that the whole scene or world uses. If you have three rulers (or axis) that form the three different dimensions, the intersection of those axis is the origin
*** What is the "default viewing system" for most 3d applications?
- If you move the apex of the viewing frustum to the origin of the world coordinate system and orient the line of sight to the negative z axis, that is what the "default viewing system" looks like
*** See image
[[file:scratchapixel-resources/box-setup4.png][box-setup4.png]] [fn:2]
*** What are similar triangles?
- Two triangles that have the same angle (between the hypotenuse and the adjacent edges). They also have another characteristic and that is that the ratio between the opposite and the adjacent edges between the two is equal. In the case of the image above that means this:

\[\frac{BC}{AB}=\frac{B\prime C\prime}{AB\prime}\] 

*** Because the canvas is 1 unit away from the origin, we know that AB' equals 1. We also know the position of B and C which are the z (depth) and y coordinates (height) respectively of the corner. If we substitute these numbers in the above equation, we get:

\[\frac{P\cdot y}{P\cdot z}=\frac{P\prime \cdot y}{1}\]

- Where y' is actually the y coordinate of the point where the line going from the corner to the viewpoint intersects the canvas, which is, was we said earlier, the dot from which we can draw an image of the box on the canvas. Thus:

\[P\prime \cdot y=\frac{P\cdot y}{P\cdot z}\]

- Given the above info, what is perspective divide?
  - In this example, it is simply the fact that projection of the y-coordinate of the corner on the canvas is the corner's y-coordinate divided by its depth (z-coordinate). The same principle applies to the x-coordinate

*** Since the z-coordinate (in this example and in many applications) is negative, the resulting x- and y-coordinate projected on the canvas will also be negative. To solve that, we simply reverse the sign of the z-coordinate. Here is the final x- and y-coordinates

- \[P\prime x=\frac{P\cdot x}{-P\cdot z}\]

- \[P\prime y=\frac{P\cdot y}{-P\cdot z}\]

*** The canvas we draw our 3d scene on is of an arbitrary size. Most computer screens aren't the same dimension, so it will be difficult to manipulate the points without knowing that in advance. What can be done to help remedy this situation?
- We simply normalize the points. We must convert the range of points possible on our arbitrary canvas to the range [0,1]
  - *Note*: I need to go back to this chapter to figure out what they are saying. In reading the tutorial on learnopengl.com it seems that normalized device coordinates (NDC) is between -1 and 1. Which if I remember correctly from reading this chapter, that's what the original coordinates the box in the scene was based on. I don't know, I'll come back to it
*** What is raster space?
- When we normalized our coordinates to be in the range [0,1], we still need actual coordinates. To do that, we simply multiply the normalized coordinates by the width or height (x * width, y * height). Now, our coordinates are in raster space
* Rendering an Image of a 3d Scene: An Overview
** It All Starts with a Computer and a Computer Screen
*** What are two popular algorithms used to solve the visibility problem?
- Ray tracing and rasterization
*** What is the major difference between the real world and computers?
- The real world is continuous whereas the computer is defined in discrete terms. This is a huge thing to understand when you try to render a 3d scene -- making something from the real world appear just as real in something artificial
*** What is the process of converting a continuous object (to a discrete one - in the case of computers for instance)?
- Discretisation
*** What does "raster" mean?
- It generally defines a grid of x and y coordinates on a display space
*** Describe how pixels encoded data for images in the early days of computers
- In the very early ages, a pixel only contained 1 bit. If that part of the image was to be black, it would be 0. White? 1
- A little bit later, each pixel contained 1 byte (8 bits). 3 bits for the red channel, 3 for the green, and 2 for the blue. This provided only 256 distinct colors (\[2^{3}*2^{3}*2^{2}\]). What if you want a color that you can't represent using those values? You find the closest one. This is called quantization
- The problem with quantization is that when we don't have enough colors, we get banding. See image:
[[file:scratchapixel-resources/posterisation.png][quantization.png]]
*** Quantization is an artifact when trying to recreate continuous data (images) on a discrete machine (computer). What's another big problem?
- Aliasing
*** What is aliasing?
- Simply put, aliasing is not being able to represent all the fine details of an image on a screen made of pixels. Let's say for example that you want to take a picture. If there is a teapot in the picture, but it's smaller than a single pixel (either a really small teapot or a really bad resolution!). One way to rectify this in the image is to just color that pixel with the single color represented by the teapot. This probably won't be accurate either, since the teapot is probably shaded which means it won't just be one single color. Obviously, you can see how this is a problem
** And It Follows with a 3d Scene
*** What is the process of converting a polygon with more than three points to a triangle?
- Triangulation
*** What is the process of placing points or vertices along a smooth surface called?
- Sampling
*** What is the process of converting a smooth surface to a triangle mesh?
- Tessalation
*** What is an implicit surface?
- A surface defined by an equation
**** What is a method to combine implicit primitives?
- CSG (Constructive Solid Geometry): a set of boolean operations is used to combine the primitives
- Metaballs (invented by Jim Blinn) is another method
*** How are NURBS define?
- With a parametric equation (see [[file:notes.org][this file]] for a bit more of an explanation)
- A sphere is defined by the following parametric equation: \[f(\theta,\phi)=(sin(\theta)cos(\phi),sin(\theta)sin(\phi),cos(\theta))\]
*** What is the problem with using implicit surfaces as a ray tracing solution?
- While there may be well defined mathematical operations to define a ray surface intersection point with an implicit surface, they still need to be converted to a mesh. Usually, it is required to use a special algorithm like Marching Cubes to do this. It could also lead to creating heavy meshes
*** Generally speaking, what is the preferred way to represent primitives in a ray tracer and why?
- When you write a ray tracer, you have to write a ray geometry intersection method for each type of primitive. If you have triangle (polygon) primitives, parametric and implicit surfaces, you have to write methods for each. Not only do you have to write intersection methods, but also other rendering functions like bloom, motion blur, displacement, etc. In conclusion, it is generally more efficient to have just one primitive that you are able to efficiently write all ray tracer functions for
* Footnotes

[fn:2] Run the command "M-x org-display-inline-images" to display the image

[fn:1] Taken from https://www.scratchapixel.com/lessons/3d-basic-rendering/introduction-to-ray-tracing/implementing-the-raytracing-algorithm
